In this directory you will find an excel spreadsheet titled "Privia Family Medicine 113018.xlsx" containing demographics,
quarter and risk data. We need this data to be manipulated and stored in our PersonDatabase for analysis.

Please include solutions to the questions below using python 2.7.
Please include any required modules in a "requirements.txt" file in this directory.
Please provide adequate test coverage for you solutions.

1. Import the 'Demographics' data section to a table in the database. This ETL will need to process files of the
same type delivered later on with different file dates and from different groups.
    a. Include all fields under 'Demographics'
    b. Define the sql schema as necessary. Fields should not include spaces or special characters.
    c. Include fields in the data table that indicate the date of the file and the provider group located in the filename.
        In this case "Privia Family Medicine" 11/30/2018. Assume the length of the group name will change and the date
        will always be formatted at the end of the file as MMDDYY
    d. Include only the first initial of the Middle Name when applicable.
    e. Convert the Sex value to M or F: M for 0 and F for 1


2. Transform and import the 'Quarters' and 'Risk' data into a separate table.
    a. Unpivot the data so that the data table includes
        i. ID
        ii. Quarter
        iii. Attributed flag
        iv. Risk Score
        v. File date
     b. Only include records in which the patients risk has increased.


Notes from Caitlin Johnson:

1. The file 'etl_process.py' contains the script for completing the above exercise; all SQL Server queries were put in
the 'queries.py' file as an effort to declutter the main script file.

2. The 'requirements.txt' file contains the modules/versions needed for the main script; however, I believe it would be
beneficial to provide context to the setup used for this coding assessment in case if it helps explain any caveats.

    a. SQL: Given that a Mac OS was utilized, Microsoft SQL Server 2017 was installed and deployed on a Docker container,
    which was then connected to Azure Data Studio.

    b. Python: PyCharm IDE was utilized for the Python portion of the exercise, which relied on 'pyodbc' for the connection
    to SQL Server on the Docker container. The following environment variables are required to be defined for the connection
    to be successful: DRIVER, PORT, DSN, USER, PWD. Depending on your setup, the environment variables may not be necessary.

3. The following files are associated with the testing scripts: 'test.py' and 'Fake Family Medicine 050119.xlsx'. I aim
for at least 80% code coverage; however, given the minimal documentation on how to test mock pyodbc connections, I could
not provide as much test coverage as I would have liked. I have experiences writing unittests that mock connections
to Snowflake and Postgres databases, for which I would assert that the mock connections were called as appropriate.